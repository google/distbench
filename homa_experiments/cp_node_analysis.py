#!/usr/bin/python3
# Copyright (c) 2020-2022 Stanford University
# Copyright (c) 2023 Google LLC
#
# Permission to use, copy, modify, and/or distribute this software for any
# purpose with or without fee is hereby granted, provided that the above
# copyright notice and this permission notice appear in all copies.
#
# THE SOFTWARE IS PROVIDED "AS IS" AND THE AUTHOR DISCLAIMS ALL WARRANTIES
# WITH REGARD TO THIS SOFTWARE INCLUDING ALL IMPLIED WARRANTIES OF
# MERCHANTABILITY AND FITNESS. IN NO EVENT SHALL THE AUTHOR BE LIABLE FOR
# ANY SPECIAL, DIRECT, INDIRECT, OR CONSEQUENTIAL DAMAGES OR ANY DAMAGES
# WHATSOEVER RESULTING FROM LOSS OF USE, DATA OR PROFITS, WHETHER IN AN
# ACTION OF CONTRACT, NEGLIGENCE OR OTHER TORTIOUS ACTION, ARISING OUT OF
# OR IN CONNECTION WITH THE USE OR PERFORMANCE OF THIS SOFTWARE.
# sudo apt getinstall python3-seaborn

import argparse
import copy
import datetime
import glob
import math
import os
import platform
import re
import shutil
import subprocess
import sys
import time
import traceback

import matplotlib
from matplotlib import cbook
from matplotlib import cm
from matplotlib.colors import LightSource
import matplotlib.pyplot as plt
import numpy as np
import pandas as pd
import seaborn as sns


# Avoid Type 3 fonts (conferences don't tend to like them).
matplotlib.rcParams['pdf.fonttype'] = 42
matplotlib.rcParams['ps.fonttype'] = 42

if platform.system() != "Windows":
    import fcntl

STYLES = ["o", "s", "v", "d", "x", "h"]


def read_rtts(file):
    """
    Read a file generated by cp_node's "dump_times" command and add its
    data to the information present in rtts.

    file:    Name of the log file.
    returns: dictionary where the keys are the lengths of the messages
    and the values are the latencies for these lengths
    """
    rtts = {}
    f = open(file, "r")
    for line in f:
        stripped = line.strip()
        if stripped[0] == '#':
            continue
        words = stripped.split()
        if (len(words) < 2):
            print("Line in %s too short (need at least 2 columns): '%s'" %
                    (file, line))
            continue
        length = int(words[0])
        usec = float(words[1])
        if length in rtts:
            rtts[length].append(usec)
        else:
            rtts[length] = [usec]
    f.close()
    return rtts

def read_files(directory, workload):
    experiments_info = {}
    for filename in os.listdir(directory):
        f = os.path.join(directory, filename)
        # checking if it is a file
        if (os.path.isfile(f) and os.path.splitext(f)[-1] == ".rtts"):
            if (os.path.splitext(f)[0].split("_")[-1] == workload):
                rtts = read_rtts(f)
                experiment = filename.removesuffix(".rtts")
                experiments_info[experiment] = rtts
        else:
            continue
    return experiments_info


def get_percentiles(experiment):
    if(experiment not in experiments_info):
        return {}
    experiment_rtts = experiments_info[experiment]
    total_latencies = []
    for rtts in experiment_rtts.values():
        total_latencies += rtts
    percentiles = {}
    percentiles["p99.9"] = np.percentile(sorted(total_latencies), 99.9)
    percentiles["p99"] = np.percentile(sorted(total_latencies), 99)
    percentiles["p90"] = np.percentile(sorted(total_latencies), 90)
    percentiles["p50"] = np.percentile(sorted(total_latencies), 50)
    percentiles["min"] = min(sorted(total_latencies))
    return percentiles

def get_buckets(rtts, total):
    """
    Generates buckets for histogramming the information in rtts.

    rtts:     A collection of message rtts, as returned by read_rtts
    total:    Total number of samples in rtts
    Returns:  A list of <length, cum_frac> pairs, in sorted order. The length
              is the largest message size for a bucket, and cum_frac is the
              fraction of all messages with that length or smaller.
    """
    buckets = []
    cumulative = 0
    for length in sorted(rtts.keys()):
        cumulative += len(rtts[length])
        buckets.append([length, cumulative/total])
    return buckets

def get_digest(experiment):
    """
    Returns an element of digest that contains data for a particular
    experiment; if this is the first request for a given experiment, the
    method reads the data for experiment and generates the digest. For
    each new digest generated, a .data file is generated in the "reports"
    subdirectory of the log directory.

    experiment:  Name of the desired experiment
    """
    digest = {}
    digest["rtts"] = {}
    digest["total_messages"] = 0
    digest["lengths"] = []
    digest["cum_frac"] = []
    digest["counts"] = []
    digest["p50"] = []
    digest["p99"] = []
    digest["p999"] = []
    digest["slow_50"] = []
    digest["slow_99"] = []
    digest["slow_999"] = []
    digest["min"] = []
    digest["slow_min"] = []

    if(not experiment in experiments_info):
        return {}
    digest["rtts"] = experiments_info[experiment]
    for key, value in digest["rtts"].items():
        digest["total_messages"] += len(value)
    rtts = digest["rtts"]
    buckets = get_buckets(rtts, digest["total_messages"])
    bucket_length, bucket_cum_frac = buckets[0]
    next_bucket = 1
    bucket_rtts = []
    bucket_slowdowns = []
    bucket_count = 0
    lengths = sorted(rtts.keys())
    lengths.append(999999999)            # Force one extra loop iteration

    for length in lengths:
        if length > bucket_length:
            digest["lengths"].append(bucket_length)
            digest["cum_frac"].append(bucket_cum_frac)
            digest["counts"].append(bucket_count)
            if len(bucket_rtts) == 0:
                bucket_rtts.append(0)
                bucket_slowdowns.append(0)
            bucket_rtts = sorted(bucket_rtts)
            digest["min"].append(bucket_rtts[0])
            digest["p50"].append(bucket_rtts[bucket_count//2])
            digest["p99"].append(bucket_rtts[bucket_count*99//100])
            digest["p999"].append(bucket_rtts[bucket_count*999//1000])
            if next_bucket >= len(buckets):
                break
            bucket_rtts = []
            bucket_slowdowns = []
            bucket_count = 0
            bucket_length, bucket_cum_frac = buckets[next_bucket]
            next_bucket += 1
        bucket_count += len(rtts[length])
        for rtt in rtts[length]:
            bucket_rtts.append(rtt)
    print("Digest finished for %s" % (experiment))

    return digest

def start_rtt_plot(title, max_y=7000, x_experiment=None, size=10,
        show_top_label=True, show_bot_label=True, figsize=[6,4],
        y_label="Latency (us)", show_upper_x_axis= True):
    """
    Create a pyplot graph that will be used for slowdown data. Returns the
    Axes object for the plot.

    title:             Title for the plot; may be empty
    max_y:             Maximum y-coordinate
    x_experiment:      Name of experiment whose rtt distribution will be used to
                       label the x-axis of the plot. None means don't label the
                       x-axis (caller will presumably invoke cdf_xaxis to do it).
    size:              Size to use for fonts
    show_top_label:    True means display title text for upper x-axis
    show_bot_label:    True means display title text for lower x-axis
    figsize:           Dimensions of plot
    y_label:           Label for the y-axis
    show_upper_x_axis: Display upper x-axis ticks and labels (percentiles)
    """

    fig = plt.figure(figsize=figsize)
    ax = fig.add_subplot(111)
    ax.autoscale(axis='y')
    if title != "":
        ax.set_title(title, size=size)
    ax.set_xlim(0, 1.0)
    # ax.set_yscale("log")
    # ax.set_ylim(100, max_y)
    ax.tick_params(right=True, which="both", direction="in", length=5)
    ticks = []
    labels = []
    y = 0
    while y <= max_y:
        ticks.append(y)
        labels.append("%d" % (y))
        y = y + 500
    ax.set_yticks(ticks)
    ax.set_yticklabels(labels, size=size)
    if show_bot_label:
        ax.set_xlabel("Message Length (bytes)", size=size)
    ax.set_ylabel(y_label, size=size)
    ax.grid(which="major", axis="y")

    if show_upper_x_axis:
        top_axis = ax.twiny()
        top_axis.tick_params(axis="x", direction="in", length=5)
        top_axis.set_xlim(0, 1.0)
        top_ticks = []
        top_labels = []
        for x in range(0, 11, 2):
            top_ticks.append(x/10.0)
            top_labels.append("%d%%" % (x*10))
        top_axis.set_xticks(top_ticks)
        top_axis.set_xticklabels(top_labels, size=size)
        if show_top_label:
            top_axis.set_xlabel("Cumulative % of Messages", size=size)
        top_axis.xaxis.set_label_position('top')

    if x_experiment != None:
        # Generate x-axis labels
        ticks = []
        labels = []
        cumulative_count = 0
        target_count = 0
        tick = 0
        digest = get_digest(x_experiment)
        rtts = digest["rtts"]
        total = digest["total_messages"]
        for length in sorted(rtts.keys()):
            cumulative_count += len(rtts[length])
            while cumulative_count >= target_count:
                ticks.append(target_count/total)
                if length < 1000:
                    labels.append("%.0f" % (length))
                elif length < 100000:
                    labels.append("%.1fK" % (length/1000))
                elif length < 1000000:
                    labels.append("%.0fK" % (length/1000))
                else:
                    labels.append("%.1fM" % (length/1000000))
                tick += 1
                target_count = (total*tick)/10
        ax.set_xticks(ticks)
        ax.set_xticklabels(labels, size=size)
    return ax

def cdf_xaxis(ax, x_values, counts, num_ticks, size=10):
    """
    Generate labels for an x-axis that is scaled nonlinearly to reflect
    a particular distribution of samples.

    ax:       matplotlib Axes object for the plot
    x:        List of x-values
    counts:   List giving the number of samples for each point in x
    ticks:    Total number of ticks go generate (including axis ends)
    size:     Font size to use for axis labels
    """

    ticks = []
    labels = []
    total = sum(counts)
    cumulative_count = 0
    target_count = 0
    tick = 0
    for (x, count) in zip(x_values, counts):
        cumulative_count += count
        while cumulative_count >= target_count:
            ticks.append(target_count/total)
            if x < 1000:
                labels.append("%.0f" % (x))
            elif x < 100000:
                labels.append("%.1fK" % (x/1000))
            elif x < 1000000:
                labels.append("%.0fK" % (x/1000))
            else:
                labels.append("%.1fM" % (x/1000000))
            tick += 1
            target_count = (total*tick)/(num_ticks-1)
    ax.set_xticks(ticks)
    ax.set_xticklabels(labels, size=size)


def make_histogram(x, y, init=None, after=True):
    """
    Given x and y coordinates, return new lists of coordinates that describe
    a histogram (transform (x1,y1) and (x2,y2) into (x1,y1), (x2,y1), (x2,y2)
    to make steps.

    x:        List of x-coordinates
    y:        List of y-coordinates corresponding to x
    init:     An optional initial point (x and y coords) which will be
              plotted before x and y
    after:    True means the horizontal line corresponding to each
              point occurs to the right of that point; False means to the
              left
    Returns:  A list containing two lists, one with new x values and one
              with new y values.
    """
    x_new = []
    y_new = []
    if init:
        x_new.append(init[0])
        y_new.append(init[1])
    for i, x_coord in enumerate(x):
        if i != 0:
            if after:
                x_new.append(x_coord)
                y_new.append(y[i-1])
            else:
                x_new.append(x[i-1])
                y_new.append(y[i])
        x_new.append(x_coord)
        y_new.append(y[i])
    return [x_new, y_new]

def plot_rtts(ax, experiment, percentile, label, **kwargs):
    digest = get_digest(experiment)
    if(not digest):
        return
    if percentile == "min":
        x, y = make_histogram(digest["cum_frac"], digest["min"],
                init=[0, digest["min"][0]], after=False)
    elif percentile == "p50":
        x, y = make_histogram(digest["cum_frac"], digest["p50"],
                init=[0, digest["p50"][0]], after=False)
    elif percentile == "p99":
        x, y = make_histogram(digest["cum_frac"], digest["p99"],
                init=[0, digest["p99"][0]], after=False)
    elif percentile == "p999":
        x, y = make_histogram(digest["cum_frac"], digest["p999"],
                init=[0, digest["p999"][0]], after=False)
    else:
        raise Exception("Bad percentile selector %s; must be p50, p99, or p999"
                % (percentile))
    ax.plot(x, y, label=label, **kwargs)

def histogram_large_messages(directory, experiment, min_size, rtts):
    x = []
    sns.set(style="darkgrid")
    for size, latencies in rtts.items():
        if size >= min_size:
            x += (latencies)
    df = pd.DataFrame(x, columns=['Latencies'])
    sns.histplot(data=df, x="Latencies", kde=True, cumulative=True).set(title=("%s messages larger than %d" % (experiment, min_size)))
    plt.savefig("%s/reports/large_messages_latencies_%s.pdf" % (directory, experiment))

def histogram_short_messages(directory, experiment, max_size, rtts):
    x = []
    for size, latencies in rtts.items():
        if size <= max_size:
            x += (latencies)
    df = pd.DataFrame(x, columns=['Latencies'])
    sns.histplot(data=df, x="Latencies", kde=True, cumulative=True).set(title=("%s messages shorter than %d" % (experiment, max_size)))
    plt.savefig("%s/reports/short_messages_latencies_%s.pdf" % (directory, experiment))

if __name__ == "__main__":
    parser = argparse.ArgumentParser()
    parser.add_argument('--directory', type = str, required = True)
    parser.add_argument('--workload', type = str, required = True)
    parser.add_argument('--gbps', type = float, required = True)
    args = parser.parse_args()
    directory = args.directory
    workload = args.workload
    gbps = args.gbps
    experiments_info = read_files(directory, workload)

    print(sorted(experiments_info.keys()))
    line_template = " {: <15} {: <15} {: <15} {: <15} {: <15} {: <}\n"
    
    for file_basename in experiments_info.keys():
        percentiles = get_percentiles(file_basename)
        if (percentiles == {}):
            continue
        digest = get_digest(file_basename)
        print("Latency summary for %s experiment:\n" % file_basename)
        print(line_template.format("samples", "min", "50%", "90%", "99%"\
                                               , "99.9%"))
        print(line_template.format(digest["total_messages"], percentiles["min"], percentiles["p50"], percentiles["p90"], percentiles["p99"]\
                                               , percentiles["p99.9"]))
        histogram_large_messages(directory, file_basename, 640000, experiments_info[file_basename])
        histogram_short_messages(directory, file_basename, 1000, experiments_info[file_basename])
    ax = start_rtt_plot("Latency vs cumulative RPC sizes %s %dGbps" % (workload, gbps))
    for file_basename in experiments_info.keys():
        plot_label = file_basename.removesuffix("_" + workload)
        plot_rtts(ax, file_basename, "p99", plot_label + "_p99")
    ax.legend(loc="upper left", prop={'size': 5})
    plt.tight_layout()
    plt.savefig("%s/reports/test_rtts_p99_%s.pdf" % (directory, workload))

